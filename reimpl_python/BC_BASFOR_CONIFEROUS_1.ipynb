{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 3\n",
    "\n",
    "from general import *\n",
    "from call import *\n",
    "\n",
    "from site_settings_coniferous_1 import *\n",
    "\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PRIOR ##\n",
    "file_prior              = '../parameters/parameters_BC_CONIFEROUS-1.txt'\n",
    "\n",
    "## DATA FILES ##\n",
    "sitedata_ancillary_file = '../data/data_ancillary_CONIFEROUS-1.txt'\n",
    "sitedata_daily_file     = '../data/data_daily_CONIFEROUS-1.txt'\n",
    "sitedata_EC_file        = '../data/data_EC_CONIFEROUS-1.txt'\n",
    "\n",
    "## COLUMN NUMBERS FOR SPECIFIC OUTPUTS ##\n",
    " # Output column numbers for DAILY data & EC data\n",
    "iWCoutput  = outputNames.index(\"WC\")\n",
    "iNEEoutput = outputNames.index(\"NEE_gCm2d\")\n",
    "iGPPoutput = outputNames.index(\"GPP_gCm2d\")\n",
    "iEToutput  = outputNames.index(\"ET_mmd\")\n",
    "\n",
    "# Indices are zero-based\n",
    "int_daily  = 30 ; nnamax_daily = 1 ; cv_daily = 0.3\n",
    "iWCdata    = 3\n",
    "# EC data:\n",
    "int_EC     = 30 ; nnamax_EC    = 1 ; cv_EC    = 0.3\n",
    "iNEEdata   = 3  ; iGPPdata     = 4 ; iETdata  = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data, slice to intervals with enough values, calc means and sds\n",
    "\n",
    "dataset_ancillary = pd.read_csv(sitedata_ancillary_file, header=None, sep=r\"\\s+\")\n",
    "dataset_daily = pd.read_csv(sitedata_daily_file, header=None, sep=r\"\\s+\")\n",
    "dataset_daily = dataset_daily.iloc[:, 1:] # drop text \"WC\"\n",
    "dataset_EC = pd.read_csv(sitedata_EC_file, header=0, sep=r\"\\s+\")\n",
    "\n",
    "@dataclass\n",
    "class Intervals:\n",
    "    mean: torch.Tensor\n",
    "    sd: torch.Tensor\n",
    "    year: torch.Tensor\n",
    "    doy: torch.Tensor\n",
    "    time: torch.Tensor\n",
    "    output_idx: int\n",
    "    interval: int\n",
    "\n",
    "    def match_output_means(self, output: torch.Tensor) -> torch.Tensor:\n",
    "        # find corresponding output intervals, calculate means\n",
    "        # Extract year, day-of-year, and data columns from the output tensor.\n",
    "        output_year = output[:, 1]\n",
    "        output_doy  = output[:, 2]\n",
    "        output_data = output[:, self.output_idx]\n",
    "        \n",
    "        # Find the index for each (year, doy) pair.\n",
    "        indices = []\n",
    "        for y, d in zip(self.year, self.doy):\n",
    "            mask = (output_year == y) & (output_doy == d)\n",
    "            idx = torch.where(mask)[0]\n",
    "            if idx.numel() == 0:\n",
    "                raise ValueError(f\"No match found for year={y}, doy={d}\")\n",
    "            # Convert to a Python integer\n",
    "            indices.append(idx[0].item())\n",
    "        \n",
    "        # Compute the mean over the interval starting at each found index.\n",
    "        means = torch.stack([torch.nanmean(output_data[i : i + self.interval]) for i in indices])\n",
    "        return means        \n",
    "    \n",
    "def intervals(dataset, interval, cv, idx) -> Intervals:\n",
    "    data = dataset.iloc[:, idx].to_numpy()\n",
    "\n",
    "    block_indices = np.arange(0, len(dataset), interval)\n",
    "    nna = np.array([np.isnan(data[i : i + interval]).sum() for i in block_indices])\n",
    "    block_indices = block_indices[nna < nnamax_daily]\n",
    "\n",
    "    data_mean = np.array([np.nanmean(data[i : i + interval]) for i in block_indices])\n",
    "    data_year = dataset.iloc[block_indices, 0].to_numpy()\n",
    "    data_doy = dataset.iloc[block_indices, 1].to_numpy()\n",
    "    data_time = data_year + (data_doy - 0.5) / 366\n",
    "\n",
    "    sdmin = cv * np.abs(data_mean)\n",
    "    data_sd = np.maximum(sdmin, 1.0)\n",
    "\n",
    "    dtype = torch.float64\n",
    "    data_mean = torch.tensor(data_mean, dtype=dtype)\n",
    "    data_sd = torch.tensor(data_sd, dtype=dtype)\n",
    "    data_year = torch.tensor(data_year, dtype=torch.int32)\n",
    "    data_doy = torch.tensor(data_doy, dtype=torch.int32)\n",
    "    data_time = torch.tensor(data_time, dtype=dtype)\n",
    "\n",
    "    return Intervals(data_mean, data_sd, data_year, data_doy, data_time, idx, interval)\n",
    "\n",
    "data_WC = intervals(dataset_daily, int_daily, cv_daily, iWCdata)\n",
    "data_NEE = intervals(dataset_EC, int_EC, cv_EC, iNEEdata)\n",
    "data_GPP = intervals(dataset_EC, int_EC, cv_EC, iGPPdata)\n",
    "data_ET = intervals(dataset_EC, int_EC, cv_EC, iETdata)\n",
    "\n",
    "dataset_ancillary = pd.read_csv(sitedata_ancillary_file, header=None, index_col=0, sep=r\"\\s+\")\n",
    "ancillary = []\n",
    "\n",
    "for name, row in dataset_ancillary.iterrows():\n",
    "    year, doy, value, sd = row\n",
    "    output_idx = outputNames.index(name)\n",
    "\n",
    "    value = torch.tensor([value], dtype=torch.float64)\n",
    "    sd = torch.tensor([sd], dtype=torch.float64)\n",
    "    year = torch.tensor([year], dtype=torch.int32)\n",
    "    doy = torch.tensor([doy], dtype=torch.int32)\n",
    "    time = year + (doy - 0.5) / 366\n",
    "\n",
    "    interval = Intervals(mean=value, sd=sd, year=year, doy=doy, time=time, output_idx=output_idx, interval=1)\n",
    "    \n",
    "    ancillary.append(interval)\n",
    "\n",
    "all_intervals = [data_WC, data_NEE, data_GPP, data_ET] + ancillary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_params_BC = pd.read_csv(file_prior, header=None, sep=r\"\\s+\", index_col=0)\n",
    "parname_BC = df_params_BC.index\n",
    "parmin_BC    = df_params_BC.iloc[:, 0].to_numpy()\n",
    "parmod_BC    = df_params_BC.iloc[:, 1].to_numpy()\n",
    "parmax_BC    = df_params_BC.iloc[:, 2].to_numpy()\n",
    "parsites_BC  = df_params_BC.iloc[:, 3].to_numpy().astype(str)\n",
    "\n",
    "ip_BC = [df_params.index.get_loc(name) for name in parname_BC]\n",
    "\n",
    "# Set parameters to mode\n",
    "params[ip_BC] = parmod_BC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.library.custom_op(\"mylib::adbasfor\", mutates_args=[], device_types=\"cpu\")\n",
    "def adbasfor(params: torch.Tensor) -> torch.Tensor:\n",
    "    assert params.device.type == \"cpu\"\n",
    "    params_np = params.numpy()\n",
    "    y = np.zeros((NDAYS, NOUT), dtype=np.float64)\n",
    "    output = submit(call_BASFOR_C, params_np, matrix_weather, calendar_fert, calendar_Ndep, calendar_prunT, calendar_thinT, NDAYS, NOUT, y)\n",
    "    return torch.from_numpy(output)\n",
    "\n",
    "\"\"\"\n",
    "@adbasfor.register_fake\n",
    "def _(params: torch.Tensor) -> torch.Tensor:\n",
    "    return params.new_empty((NDAYS, NOUT))\n",
    "\"\"\"\n",
    "\n",
    "def backward(ctx, grad_output):\n",
    "    input, = ctx.saved_tensors\n",
    "    \n",
    "    grad_input = submit(call_dBASFOR_C, input.detach(), matrix_weather, calendar_fert, calendar_Ndep, calendar_prunT, calendar_thinT, NDAYS, NOUT, grad_output)\n",
    "\n",
    "    return torch.from_numpy(grad_input[0][:input.size(0)])\n",
    "\n",
    "def setup_context(ctx, inputs, output):\n",
    "    # FIXME\n",
    "    ctx.save_for_backward(inputs[0])\n",
    "\n",
    "adbasfor.register_autograd(backward, setup_context=setup_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "half_log_2pi = torch.tensor(0.5 * np.log(2 * np.pi))\n",
    "\n",
    "def flogL(sims: torch.Tensor, data: torch.Tensor, data_s: torch.Tensor):\n",
    "    Ri = (sims - data) / data_s\n",
    "    i0 = torch.abs(Ri) < 1.e-8\n",
    "\n",
    "    logLi = torch.log(1 - torch.exp(-0.5 * Ri**2)) - torch.log(Ri**2) - torch.log(data_s)\n",
    "\n",
    "    logLi[i0] =  - torch.log(2 * data_s[i0])\n",
    "    logLi -= half_log_2pi\n",
    "    return torch.sum(logLi)\n",
    "\n",
    "def loss(output):\n",
    "    logLikelihoods = []\n",
    "    for interval in all_intervals:\n",
    "        sims = interval.match_output_means(output)\n",
    "        data = interval.mean\n",
    "        data_s = interval.sd\n",
    "        logLikelihoods.append(flogL(sims, data, data_s))\n",
    "\n",
    "    logL = torch.sum(torch.stack(logLikelihoods))\n",
    "    return -logL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factors = torch.ones(len(params), dtype=torch.float64, requires_grad=True)\n",
    "optim = torch.optim.Adam([factors], lr=1e-1)\n",
    "\n",
    "for i in range(20):\n",
    "    optim.zero_grad()\n",
    "    params_tensor = factors*torch.tensor(params)\n",
    "    output = adbasfor(params_tensor)\n",
    "    loss_value = loss(output)\n",
    "    loss_value.backward()\n",
    "    optim.step()\n",
    "    with torch.no_grad():\n",
    "        factors.clamp_(0.2, 5)\n",
    "    print(loss_value.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
